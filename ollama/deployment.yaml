apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: ollama
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: ollama
  # Only run one at once, mostly because it's memory hungry. We can take an outage at home.
  replicas: 1
  revisionHistoryLimit: 3
  # During deployments, don't start another before the first is terminated.
  strategy:
    type: Recreate 
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ollama
    spec:
      # Switch to the nvidia runtime class to get access to the GPU
      runtimeClassName: nvidia
      # Since we have both a 1060 and a 3060 in the cluster, ensure we run on the 
      # faster one by selecting the node with it in. I can't do this via auto generated
      # labels it seems - I could add the label, but this is less work)
      nodeName: crobceratops
      containers:
      - name: ollama
        image: ollama/ollama:0.3.11 # https://hub.docker.com/r/ollama/ollama/tags
        # Ensure we run on a node with GPU available
        resources:
          limits:
            nvidia.com/gpu: '1'
        ports:
        - name: ollama
          containerPort: 11434
        volumeMounts:
        - name: storage
          # Models are downloaded and stored in the root home directory
          mountPath: /root/.ollama
        # Full inspiration from https://github.com/otwld/ollama-helm/blob/main/templates/deployment.yaml#L140-L150
        # I just didn't want to run the helm chart...
        lifecycle:
          postStart:
            exec:
              command:
                - /bin/sh
                - -c
                - | 
                  while ! /bin/ollama ps > /dev/null 2>&1; do
                    sleep 5
                  done
                  /bin/ollama pull llama3.1
                  /bin/ollama pull deepseek-coder-v2
                  /bin/ollama pull nomic-embed-text
                  /bin/ollama pull snowflake-arctic-embed
                  /bin/ollama pull mxbai-embed-large
      volumes:
      - name: storage
        hostPath:
          path: /stash/ollama
          type: DirectoryOrCreate # if the directory does not exist then create it