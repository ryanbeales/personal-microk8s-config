apiVersion: apps/v1
kind: Deployment
metadata:
  name: piper
  namespace: piper
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: piper
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    type: Recreate 
  template:
    metadata:
      labels:
        app.kubernetes.io/name: piper
    spec:
      # Switch to the nvidia runtime class to get access to the GPU
      runtimeClassName: nvidia
      
      # This process is tiny, we have a 1050Ti-4GB, 1060-6GB, and 3060-12GB in the cluster. We won't restrict this to a specific node.
      nodeSelector:
        nvidia.com/gpu.present: "true"

      containers:
      - name: piper
        # Disable - renovate: datasource=github-tags depname=linuxserver/docker-piper versioning=docker
        image: lscr.io/linuxserver/piper:1.6.3-gpu # https://hub.docker.com/r/linuxserver/piper/tags
        env:
        - name: PUID
          value: "1000"
        - name: PGID  
          value: "1000"
        - name: TZ
          value: America/Vancouver
        - name: PIPER_VOICE
          value: en_US-lessac-medium
        resources:
          limits:
            nvidia.com/gpu: '1'
        ports:
        - name: piper
          containerPort: 10200
        volumeMounts:
        - name: storage
          # Models are downloaded and stored in the root home directory
          mountPath: /config
      volumes:
      - name: storage
        persistentVolumeClaim:
          claimName: piper-nfs-pvc