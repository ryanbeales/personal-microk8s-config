apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: ollama
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: ollama
  # Only run one at once, mostly because it's memory hungry. We can take an outage at home.
  replicas: 1
  revisionHistoryLimit: 3
  # During deployments, don't start another before the first is terminated.
  strategy:
    type: Recreate 
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ollama
    spec:
      # Switch to the nvidia runtime class to get access to the GPU
      runtimeClassName: nvidia
      # Run on 3060 GPU node
      nodeName: crobceratops
      containers:
      - name: ollama
        # renovate: datasource=github-tags depname=ollama/ollama versioning=semver
        image: ollama/ollama:0.13.4
        env:
        # Allow from everywhere (we limit it via where it's served from)
        - name: OLLAMA_ORIGINS
          value: "*"
        # Ensure we run on a node with GPU available
        resources:
          limits:
            nvidia.com/gpu: '1'
        ports:
        - name: ollama
          containerPort: 11434
        volumeMounts:
        - name: storage
          # Models are downloaded and stored in the root home directory
          mountPath: /root/.ollama
        # Full inspiration from https://github.com/otwld/ollama-helm/blob/main/templates/deployment.yaml#L140-L150
        # I just didn't want to run the helm chart...
        lifecycle:
          postStart:
            exec:
              command:
                - /bin/sh
                - -c
                - | 
                  while ! /bin/ollama ps > /dev/null 2>&1; do
                    sleep 5
                  done
                  /bin/ollama pull llama3.1
                  /bin/ollama pull gemma3:4b-it-qat
      volumes:
      - name: storage
        persistentVolumeClaim:
          claimName: ollama-nfs-pvc